import numpy as np

class Perceptron:
      def __init__(self, input_dim, output_dim):
          self.weights = np.random.rand(input_dim, output_dim)
          self.bias = np.random.rand(output_dim)
      
      def forward(self, X):
          logits = np.dot(X, self.weights) + self.bias
          return np.where(logits >= 0, 1, 0)
      
      def compute_loss(self, X, y_true):
          y_pred = self.forward(X)
          loss = np.mean((y_pred != y_true).astype(float))
          return loss
          class GeneticAlgorithm:
      def __init__(self, pop_size, gene_length, mutation_rate=0.01):
          self.pop_size = pop_size
          self.gene_length = gene_length
          self.mutation_rate = mutation_rate
      
      def initialize_population(self):
          return np.random.rand(self.pop_size, self.gene_length)
      
      def select_parents(self, population, losses):
          # Метод турнирного отбора
          indices = np.argsort(losses)
          top_half = indices[:len(indices)//2]
          return population[top_half]
      
      def crossover(self, parent1, parent2):
          split_point = np.random.randint(1, self.gene_length)
          child1 = np.concatenate((parent1[:split_point], parent2[split_point:]))
          child2 = np.concatenate((parent2[:split_point], parent1[split_point:]))
          return child1, child2
      
      def mutate(self, chromosome):
          mask = np.random.rand(self.gene_length) < self.mutation_rate
          mutations = np.random.rand(self.gene_length)
          chromosome[mask] += mutations[mask]
          return chromosome
      
      def evolve(self, population, losses):
          new_population = []
          parents = self.select_parents(population, losses)
          while len(new_population) < self.pop_size:
              parent1, parent2 = np.random.choice(parents, 2, replace=False)
              child1, child2 = self.crossover(parent1, parent2)
              child1 = self.mutate(child1)
              child2 = self.mutate(child2)
              new_population.extend([child1, child2])
          return np.array(new_population[:self.pop_size])
          # Подготовим данные
X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])  # входные данные
y = np.array([0, 1, 1, 0])                     # ожидаемые выходные данные

input_dim = X.shape[1]
output_dim = 1

# Создаём перцептрон
perceptron = Perceptron(input_dim, output_dim)

# Создаём генетический алгоритм
ga = GeneticAlgorithm(pop_size=100, gene_length=input_dim*output_dim+output_dim)

# Основная петля обучения
for generation in range(100):
    # Извлекаем текущих кандидатов (популяцию)
    weights_biases = ga.initialize_population()
    
    # Оцениваем каждую особь
    losses = []
    for wb in weights_biases:
        w = wb[:-output_dim].reshape(input_dim, output_dim)
        b = wb[-output_dim:]
        perceptron.weights = w
        perceptron.bias = b
        loss = perceptron.compute_loss(X, y)
        losses.append(loss)
    
    # Эволюция следующего поколения
    new_generation = ga.evolve(weights_biases, losses)
    weights_biases = new_generation
    
    # Показываем лучший результат текущего поколения
    best_idx = np.argmin(losses)
    print(f"Generation {generation}, Best loss: {losses[best_idx]:.4f}")

    #многослойный
    class MultilayerPerceptron:
    def __init__(self, input_dim, hidden_dims, output_dim):
        dims = [input_dim] + hidden_dims + [output_dim]
        self.weights = [np.random.rand(dims[i], dims[i+1]) for i in range(len(dims)-1)]
        self.biases = [np.random.rand(dims[i+1]) for i in range(len(dims)-1)]
    
    def forward(self, X):
        a = X
        for w, b in zip(self.weights, self.biases):
            z = np.dot(a, w) + b
            a = np.maximum(z, 0)  # ReLU активация
        return a
    
    def compute_loss(self, X, y_true):
        y_pred = self.forward(X)
        loss = np.mean((y_pred != y_true).astype(float))
        return loss
        class GAForMultilayer(GeneticAlgorithm):
    def __init__(self, pop_size, input_dim, hidden_dims, output_dim, mutation_rate=0.01):
        gene_length = sum((d_in*d_out+d_out for d_in, d_out in zip([input_dim]+hidden_dims, hidden_dims+[output_dim])))
        super().__init__(pop_size, gene_length, mutation_rate)
    
    def extract_parameters(self, chromosome, input_dim, hidden_dims, output_dim):
        params = []
        start = 0
        for d_in, d_out in zip([input_dim]+hidden_dims, hidden_dims+[output_dim]):
            end = start + d_in*d_out
            weights = chromosome[start:end].reshape(d_in, d_out)
            bias_start = end
            bias_end = bias_start + d_out
            biases = chromosome[bias_start:bias_end]
            params.append((weights, biases))
            start = bias_end
        return params
        # Подготовим данные
X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])  # входные данные
y = np.array([0, 1, 1, 0])                     # ожидаемые выходные данные

input_dim = X.shape[1]
hidden_dims = [4]
output_dim = 1

# Создаём многослойный перцептрон
mlp = MultilayerPerceptron(input_dim, hidden_dims, output_dim)

# Создаём генетический алгоритм
ga = GAForMultilayer(pop_size=100, input_dim=input_dim, hidden_dims=hidden_dims, output_dim=output_dim)

# Основная петля обучения
for generation in range(100):
    # Извлекаем текущих кандидатов (популяцию)
    weights_biases = ga.initialize_population()
    
    # Оцениваем каждую особь
    losses = []
    for wb in weights_biases:
        params = ga.extract_parameters(wb, input_dim, hidden_dims, output_dim)
        mlp.weights, mlp.biases = list(zip(*params))
        loss = mlp.compute_loss(X, y)
        losses.append(loss)
    
    # Эволюция следующего поколения
    new_generation = ga.evolve(weights_biases, losses)
    weights_biases = new_generation
    
    # Показываем лучший результат текущего поколения
    best_idx = np.argmin(losses)
    print(f"Generation {generation}, Best loss: {losses[best_idx]:.4f}")
    
